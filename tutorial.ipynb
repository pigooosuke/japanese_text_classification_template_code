{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tutorial for this script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import CaboCha\n",
    "\n",
    "# NGワードがあれば追加\n",
    "NG_WORDS=[\n",
    "# \"AAA\",\"BBB\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_word(text):\n",
    "    \"\"\"\n",
    "    textを受け取り、\n",
    "    ・内容語（名詞・動詞・形容詞）\n",
    "    ・名詞句（名詞が続く）\n",
    "    ・内容語が含まれるbi-gram\n",
    "    を抽出し、listで返す\n",
    "    \"\"\"\n",
    "    # 名詞句調査\n",
    "    is_prefix = 0 # 接頭詞の発見フラグ\n",
    "    is_prefix_and_noun = 0 # 接頭詞に続く名詞の発見フラグ\n",
    "    cnt_head_noun = 0 # 名詞句の構成単語が何単語含まれるか\n",
    "    tmp_noun_phrase = \"\" # 名詞句の一時保存\n",
    "    # 内容語のbigram調査\n",
    "    is_main_word = 0 # 内容語であるか\n",
    "    is_main_word_before_word = 0 # 前の単語に内容語があったか\n",
    "    tmp_bigram_word = \"\" # bigramの一時保存\n",
    "    # return value list\n",
    "    token_main_words = []\n",
    "    token_prefix_and_noun = []\n",
    "    token_bigram = []\n",
    "\n",
    "    mecab = MeCab.Tagger()\n",
    "    mecab.parse(\"\")\n",
    "    node = mecab.parseToNode(text)\n",
    "    while node:\n",
    "        # リセット\n",
    "        skip = 0\n",
    "        # NG ワード確認\n",
    "        if node.surface in NG_WORDS:\n",
    "            is_main_word = 0\n",
    "        else:\n",
    "            # 内容語抽出\n",
    "            if node.feature.split(\",\")[0] in [\"名詞\",\"動詞\",\"形容詞\"] and node.feature.split(\",\")[1] != \"数\":\n",
    "                token_main_words.append(node.surface)\n",
    "                is_main_word = 1\n",
    "            else:\n",
    "                is_main_word = 0\n",
    "            # 名詞句抽出\n",
    "            if node.feature.split(\",\")[0] == \"接頭詞\":\n",
    "                is_prefix = 1\n",
    "                tmp_noun_phrase += node.surface\n",
    "            elif node.feature.split(\",\")[0] == \"名詞\":\n",
    "                if cnt_head_noun >= 1 or is_prefix == 1:\n",
    "                    tmp_noun_phrase += node.surface\n",
    "                    cnt_head_noun += 1\n",
    "                else:\n",
    "                    cnt_head_noun += 1\n",
    "                    tmp_noun_phrase += node.surface\n",
    "                is_prefix = 0\n",
    "            else:\n",
    "                if cnt_head_noun > 1:\n",
    "                    token_prefix_and_noun.append(tmp_noun_phrase)\n",
    "                    is_prefix = 0\n",
    "                    cnt_head_noun = 0\n",
    "                    tmp_noun_phrase = \"\"\n",
    "                else:\n",
    "                    is_prefix = 0\n",
    "                    cnt_head_noun = 0\n",
    "                    tmp_noun_phrase = \"\"\n",
    "            # 内容語のbigram抽出\n",
    "            if tmp_bigram_word:\n",
    "                if node.surface == \"\" or node.feature.split(\",\")[0] ==\"記号\":\n",
    "                    pass\n",
    "                elif is_main_word_before_word == 1:\n",
    "                    token_bigram.append(tmp_bigram_word + node.surface)\n",
    "                elif is_main_word == 1:\n",
    "                    token_bigram.append(tmp_bigram_word + node.surface)\n",
    "                else:\n",
    "                    pass\n",
    "            # 最終処理-内容語のbigram抽出-\n",
    "            tmp_bigram_word = node.surface\n",
    "            if is_main_word == 1:\n",
    "                is_main_word_before_word = 1\n",
    "            else:\n",
    "                is_main_word_before_word = 0\n",
    "        node = node.next\n",
    "\n",
    "    return token_main_words, token_prefix_and_noun, token_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_main_words:\n",
      "['広告', '代理', '店', '勤務', 'する', 'マサエ', 'さん', '歳', 'お気に入り', '位', '回', '生き', '猫']\n",
      "token_prefix_and_noun:\n",
      "['広告代理店', 'マサエさん', '40歳', 'お気に入り第二位', '100万回']\n",
      "token_bigram:\n",
      "['広告代理', '代理店', '店に', 'に勤務', '勤務する', 'するマサエ', 'マサエさん', '40歳', 'のお気に入り', 'お気に入り第', '二位', '位は', '万回', '回生き', '生きた', 'た猫']\n"
     ]
    }
   ],
   "source": [
    "text=\"広告代理店に勤務するマサエさん（40歳）のお気に入り第二位は『100万回生きた猫』\"\n",
    "token_main_words, token_prefix_and_noun, token_bigram = extract_word(text)\n",
    "print(\"token_main_words:\")\n",
    "print(token_main_words)\n",
    "print(\"token_prefix_and_noun:\")\n",
    "print(token_prefix_and_noun)\n",
    "print(\"token_bigram:\")\n",
    "print(token_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_dependency(text):\n",
    "    \"\"\"\n",
    "    渡されたテキスト文に係り受けのペアを返す\n",
    "    名詞-動詞\n",
    "    \"\"\"\n",
    "    cabocha = CaboCha.Parser()\n",
    "    tree = cabocha.parse(text)\n",
    "    chunk_dic = {}\n",
    "    chunk_id = 0\n",
    "    for i in range(0, tree.size()):\n",
    "        token = tree.token(i)\n",
    "        if token.chunk:\n",
    "            chunk_dic[chunk_id] = token.chunk\n",
    "            chunk_id += 1\n",
    "    dependency_token = []\n",
    "    for chunk_id, chunk in chunk_dic.items():\n",
    "        if chunk.link > 0:\n",
    "            from_surface, from_feature =  word_dependency_get_word(tree, chunk)\n",
    "            to_chunk = chunk_dic[chunk.link]\n",
    "            to_surface, to_feature = word_dependency_get_word(tree, to_chunk)\n",
    "            if from_feature != to_feature and from_feature != \"形容詞\" \\\n",
    "                and to_feature != \"形容詞\" and from_feature != \"\":\n",
    "                dependency_token.append(from_surface + \" \" + to_surface)\n",
    "    return dependency_token\n",
    "\n",
    "def word_dependency_get_word(tree, chunk):\n",
    "    \"\"\"\n",
    "    word_dependency中の処理\n",
    "    係り受けとなっている単語の品詞を特定する\n",
    "    \"\"\"\n",
    "    surface = ''\n",
    "    feature = ''\n",
    "    for i in range(chunk.token_pos, chunk.token_pos + chunk.token_size):\n",
    "        token = tree.token(i)\n",
    "        features = token.feature.split(',')\n",
    "        if features[0] == '名詞':\n",
    "            surface += token.surface\n",
    "            feature = '名詞'\n",
    "        elif features[0] == '形容詞':\n",
    "            surface += features[6]\n",
    "            break\n",
    "        elif features[0] == '動詞':\n",
    "            surface += features[6]\n",
    "            feature = '動詞'\n",
    "            break\n",
    "    return surface, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dependency_token:\n",
      "['広告代理店 勤務する', '勤務する マサエさん', '100万回 生きる', '生きる 猫']\n"
     ]
    }
   ],
   "source": [
    "dependency_token = word_dependency(text)\n",
    "print(\"dependency_token:\")\n",
    "print(dependency_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ライブラリの読み込み。  \n",
    "log用のスクリプト設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "logging.basicConfig(filename='result.log',level=logging.INFO)\n",
    "\n",
    "def log(content):\n",
    "    time = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "    print (time + ': ' + content)\n",
    "    logging.info(time + ': ' + content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプル用にニュースサイトのテキストデータを取得する  \n",
    "https\\://www.rondhuit.com/download.html  \n",
    "同一フォルダ内に解凍したデータを置く  \n",
    "\n",
    "今回は、「peachy」「sports-watch」の２カテゴリを調査  \n",
    "各200文書を取得  \n",
    "\n",
    "==peachy==  \n",
    "各記事ファイルにはクリエイティブ・コモンズライセンス「表示 – 改変禁止」が適用されます。 クレジット表示についてはニュースカテゴリにより異なるため、ダウンロードしたファイルを展開したサブディレクトリにあるそれぞれの LICENSE.txt をご覧ください。 livedoor はNHN Japan株式会社の登録商標です。  \n",
    "==sports-watch==  \n",
    "このディレクトリにあるすべての記事ファイルには、クリエイティブ・コモンズライセンス「表示 - 改変禁止」（http\\://creativecommons.org/licenses/by-nd/2.1/jp/）が適用されます。原著作者のクレジットを表示し、ニュース記事の改変をしないことを条件に、記事全文を自由に転載・引用が可能です。\n",
    "このディレクトリの記事ファイル内容の提供元：Sports Watch http\\://news.livedoor.com/category/vender/208/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "TARGET_CATEGORY=[\"peachy\",\"sports-watch\"]\n",
    "\n",
    "def read_news():\n",
    "    \"\"\"\n",
    "    サンプルデータの読み込み\n",
    "    \"\"\"\n",
    "    with open(\"example.csv\",\"w\") as wf:\n",
    "        writer = csv.writer(wf)\n",
    "        for category in TARGET_CATEGORY:\n",
    "            files = os.listdir(os.path.join('text',category))\n",
    "            cnt=0\n",
    "            for file in files:\n",
    "                contents =\"\"\n",
    "                tokens=[]\n",
    "                if file==\"LICENSE.txt\" or cnt>200:\n",
    "                    continue\n",
    "                with open(os.path.join(\"text\",category,file),\"r\") as rf:\n",
    "                    reader = csv.reader(rf)\n",
    "                    for row in reader:\n",
    "                        if not row:\n",
    "                            continue\n",
    "                        contents += row[0]\n",
    "                        token_main_words, token_prefix_and_noun, token_bigram = extract_word(row[0])\n",
    "                        token = token_main_words + token_prefix_and_noun\n",
    "                        tokens.extend(token)\n",
    "                    concated_tokens=\" \".join(tokens)\n",
    "                    writer.writerow([contents, concated_tokens, category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# データの取得\n",
    "read_news()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先ほど抽出したニュースデータを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    import dataset\n",
    "    ## example.csv\n",
    "    row#1: raw contents\n",
    "    row#2: labels data\n",
    "    row#3: tokenized contents (tfidf:separeted by space)\n",
    "    \"\"\"\n",
    "    contents=[]\n",
    "    labels=[]\n",
    "    tokens=[]\n",
    "    with open(\"example.csv\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        for row in reader:\n",
    "            contents.append(row[0])\n",
    "            tokens.append(row[1])\n",
    "            labels.append(row[2])\n",
    "\n",
    "    return contents, tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents, tokens, labels = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:\n",
      "peachy\n",
      "\n",
      "contents:\n",
      "http://news.livedoor.com/article/detail/6732572/2012-07-07T18:00:00+0900−31キロを実現したダイエット法／美肌を作る30秒マッサージなど−【ビューティー】週間ランキング美しさを追求するのは、女子の“永遠の課題”と言っても過言ではありません。「ファッション・ビューティ」カテゴリの中から、Peachyのアプリで先週（2012年6月28日〜7月4日）最も読まれた記事TOP5をお届けします！第1位：あびる優、体重を公表　スレンダーボディの秘訣明かすタレントのあびる優が、オフィシャルブログにてファンから寄せられた質問に回答。スレンダーボディをキープする秘訣を明かした。第2位：女性のムダ毛にガッカリ…7割の男性は女性のムダ毛に気づいている！薄着になり、水着が店頭に並び、女性がダイエットを意識する夏前のこの季節、ダイエット以外にも意識しておきたいことがある。それが「ムダ毛処理」だ第3位：藤井リナ、すっぴん美肌の秘訣を伝授雑誌「VoCE」8月号に、人気モデルの藤井リナが登場した。28歳を目前にした今、これまでの“カワイイ”に留まらずエロさ、毒、愛らしさを秘めた大人の魅力を放ち始めた藤井リナ。そんな彼女の記念すべき「VoCE」初インタビューでは、安室奈美恵ら数多くの有名人を手がけるヘアメイク・中野明海さんとの初セッションが実現。スキンケア、スタイルキープ法、ストレス解消法など、ビューティーにまつわる様々なこだわりを明かしている。第4位：一日の美肌の鍵は「朝の30秒マッサージ」だった！きゃ〜、美容に時間をかけていたら遅刻しちゃう！　一分一秒でも惜しい、忙しい朝、みなさんはどんなスキンケアをしていますか？　また、朝のスキンケアは、夜のスキンケアと比べて“手をかけるレベル”はいかがでしょうか？株式会社ネオマーケティングが2012年6月に20歳〜30歳代の女性1第5位：−31kgの重量級痩せに成功！そのダイエット法とは雑誌「Popteen」8月号の特集「ガチやせDIET総選挙2012・夏」では、−31kgを筆頭とした最強ヤセ読者たちのダイエット術を大解剖。まるで別人へと生まれ変わった彼女たちの、壮絶な努力の物語が語られている。以上、先週の「ビューティ」カテゴリの人気記事ランキングでした！\n",
      "\n",
      "tokens:\n",
      "['http', 'news', 'livedoor', 'com', 'article', 'detail', 'T', '07T18', 'キロ', '実現', 'し', 'ダイエット', '法', '美肌', '作る', '秒', 'マッサージ', 'ビューティー', '週間', 'ランキング', '31キロ', 'ダイエット法', '30秒マッサージ', '週間ランキング', '美し', 'さ', '追求', 'する', 'の', '女子', '永遠', '課題', '言っ', '過言', 'あり', 'ファッション・ビューティ', 'カテゴリ', '中', 'Peachy', 'アプリ', '先週', '年', '月', '日', '月', '日', '読ま', 'れ', '記事', 'TOP', '届け', 'し', '2012年6月28日', '7月4日', '記事TOP5', '位', 'あびる', '優', '体重', '公表', 'スレンダーボディ', '秘訣', '明かす', '第1位', 'タレント', 'あびる', '優', 'オフィシャルブログ', 'ファン', '寄せ', 'られ', '質問', '回答', 'スレンダーボディ', 'キープ', 'する', '秘訣', '明かし', '位', '女性', 'ムダ', '毛', '割', '男性', '女性', 'ムダ', '毛', '気づい', 'いる', '第2位', 'ムダ毛', '7割', 'ムダ毛', '薄着', 'なり', '水着', '店頭', '並び', '女性', 'ダイエット', '意識', 'する', '夏', '前', '季節', 'ダイエット', '以外', '意識', 'し', 'おき', 'こと', 'ある', 'それ', 'ムダ', '毛', '処理', '夏前', 'ダイエット以外', 'ムダ毛処理', '位', '藤井', 'リナ', 'すっぴん', '美肌', '秘訣', '伝授', '第3位', '藤井リナ', 'すっぴん美肌', '雑誌', 'VoCE', '月', '号', '人気', 'モデル', '藤井', 'リナ', '登場', 'し', '歳', '目前', 'し', '今', 'これ', 'カワイイ', '留まら', 'エロ', 'さ', '毒', '愛らし', 'さ', '秘め', '大人', '魅力', '放ち', '始め', '藤井', 'リナ', '彼女', '記念', 'す', 'VoCE', 'インタビュー', '安室', '奈美恵', 'ら', '多く', '有名人', '手がける', 'ヘア', 'メイク', '中野', '明海', 'さん', 'セッション', '実現', 'スキン', 'ケア', 'スタイル', 'キープ', '法', 'ストレス', '解消', '法', 'ビューティー', '様々', 'こだわり', '明かし', 'いる', '8月号', '人気モデル', '藤井リナ', '28歳', 'エロさ', '藤井リナ', '安室奈美恵ら数多く', 'ヘアメイク', '中野明海さん', 'スキンケア', 'スタイルキープ法', 'ストレス解消法', '位', '日', '美肌', '鍵', '朝', '秒', 'マッサージ', '第4位', '一日', '30秒マッサージ', 'きゃ', '美容', '時間', 'かけ', 'い', '遅刻', 'し', 'ちゃう', '分', '秒', '惜しい', '忙しい', '朝', 'みなさん', 'スキン', 'ケア', 'し', 'い', '朝', 'スキン', 'ケア', '夜', 'スキン', 'ケア', '比べ', '手', 'かける', 'レベル', 'いかが', '株式会社', 'ネオ', 'マーケティング', '年', '月', '歳', '歳', '代', '女性', '一分一秒', 'スキンケア', 'スキンケア', 'スキンケア', '株式会社ネオマーケティング', '2012年6月', '20歳', '30歳代', '女性1', '位', 'kg', '重量', '級', '痩せ', '成功', 'ダイエット', '法', '第5位', '31kg', '重量級', 'ダイエット法', '雑誌', 'Popteen', '月', '号', '特集', 'ガチ', 'DIET', '選挙', '夏', 'kg', '筆頭', 'し', '最強', 'ヤセ', '読者', 'たち', 'ダイエット', '術', '解剖', '別人', '生まれ変わっ', '彼女', 'たち', '壮絶', '努力', '物語', '語ら', 'れ', 'いる', '8月号', 'DIET総選挙2012・夏', '31kg', '最強ヤセ読者たち', 'ダイエット術', '彼女たち', '以上', '先週', 'ビューティ', 'カテゴリ', '人気', '記事', 'ランキング', '人気記事ランキング']\n"
     ]
    }
   ],
   "source": [
    "print(\"labels:\")\n",
    "print(labels[0])\n",
    "print()\n",
    "print(\"contents:\")\n",
    "print(contents[0])\n",
    "print()\n",
    "print(\"tokens:\")\n",
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_labels(labels):\n",
    "    \"\"\"\n",
    "    教師ラベルをシリアル変換\n",
    "    category_list:カテゴリの位置に対応した名称を保存\n",
    "    \"\"\"\n",
    "    # カテゴリ名取得、シリアル変換\n",
    "    l_encoder = LabelEncoder()\n",
    "    l_encoder.fit(labels)\n",
    "    category_list = list(l_encoder.classes_)\n",
    "    serialized_labels = l_encoder.transform(labels)\n",
    "\n",
    "    return serialized_labels, category_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ラベルデータのシリアライズ化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modified_labels, category_list = format_labels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified_labels:\n",
      "[0 0 0 ..., 1 1 1]\n",
      "\n",
      "category_list:\n",
      "['peachy', 'sports-watch']\n"
     ]
    }
   ],
   "source": [
    "print(\"modified_labels:\")\n",
    "print(modified_labels)\n",
    "print()\n",
    "print(\"category_list:\")\n",
    "print(category_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ラベルの情報  \n",
    "0:peachy  \n",
    "1:sports-watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def execute_model(pipeline, params, contents, tokens, label, category_list):\n",
    "    \"\"\"\n",
    "    classification 実行\n",
    "    \"\"\"\n",
    "    log(\"-------------------------\")\n",
    "    log(\"----- Score  Report -----\")\n",
    "    log(\"-------------------------\")\n",
    "    t0 = time()\n",
    "    train_x, test_x, train_y, test_y = train_test_split(tokens, label, test_size=0.2, random_state=22)\n",
    "    train_x_raw, test_x_raw, train_y_raw, test_y_raw = train_test_split(contents, label, test_size=0.2, random_state=22)\n",
    "    clf = GridSearchCV(pipeline, params)\n",
    "    clf.fit(train_x,train_y)\n",
    "    log(\"done in %0.3fs\" % (time() - t0))\n",
    "    log(\"score %f\" % (clf.score(train_x,train_y)))\n",
    "    \n",
    "    # グリッドサーチをしないときは以下の部分をコメントアウトする\n",
    "    log(\"Best parameters set:\")\n",
    "    best_parameters = clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(params.keys()):\n",
    "        log(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    pred = clf.predict(test_x)\n",
    "    confidence = clf.predict_proba(test_x)\n",
    "    # スコア計算\n",
    "    F1_score = f1_score(test_y, pred, average=\"weighted\")\n",
    "    accur = accuracy_score(test_y, pred)\n",
    "    log(\"F1_score: %f\" % F1_score)\n",
    "    log(\"ACCUR: %f\" % accur)\n",
    "    log(classification_report(test_y, pred, target_names=category_list))\n",
    "    log(\"done in %0.3fs\" % (time() - t0))\n",
    "    # joblib.dump(clf, 'clf.pkl')\n",
    "\n",
    "    with open(\"predict_result.csv\",\"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        for (x,y,p,c) in zip(test_x_raw,test_y,pred,confidence):\n",
    "            writer.writerow([x,y,p,max(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# グリッドサーチ用のパラメーター\n",
    "params={\n",
    "'lr__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "'vectorizer__norm': [None, 'l2'],\n",
    "}\n",
    "pipeline = Pipeline([\n",
    "('vectorizer', TfidfVectorizer()),\n",
    "('lr', LogisticRegression(class_weight = \"balanced\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017/08/31 13:45:21: -------------------------\n",
      "2017/08/31 13:45:21: ----- Score  Report -----\n",
      "2017/08/31 13:45:21: -------------------------\n",
      "2017/08/31 13:46:00: done in 38.997s\n",
      "2017/08/31 13:46:01: score 1.000000\n",
      "2017/08/31 13:46:01: Best parameters set:\n",
      "2017/08/31 13:46:01: \tlr__C: 1000\n",
      "2017/08/31 13:46:01: \tvectorizer__norm: 'l2'\n",
      "2017/08/31 13:46:01: F1_score: 0.994268\n",
      "2017/08/31 13:46:01: ACCUR: 0.994269\n",
      "2017/08/31 13:46:01:               precision    recall  f1-score   support\n",
      "\n",
      "      peachy       0.99      1.00      0.99       181\n",
      "sports-watch       1.00      0.99      0.99       168\n",
      "\n",
      " avg / total       0.99      0.99      0.99       349\n",
      "\n",
      "2017/08/31 13:46:01: done in 39.780s\n"
     ]
    }
   ],
   "source": [
    "execute_model(pipeline, params, contents, tokens, modified_labels, category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"predict_result.csv\",names=(\"contents\",\"labels\",\"predict\",\"confidence\"))\n",
    "#見やすいように置換\n",
    "mapping={0:'peachy', 1:'sports-watch'}\n",
    "df[[\"labels\",\"predict\"]] = df[[\"labels\",\"predict\"]].replace(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "      <th>labels</th>\n",
       "      <th>predict</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://news.livedoor.com/article/detail/474526...</td>\n",
       "      <td>sports-watch</td>\n",
       "      <td>sports-watch</td>\n",
       "      <td>0.988596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://news.livedoor.com/article/detail/591090...</td>\n",
       "      <td>peachy</td>\n",
       "      <td>peachy</td>\n",
       "      <td>0.999831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://news.livedoor.com/article/detail/491990...</td>\n",
       "      <td>sports-watch</td>\n",
       "      <td>sports-watch</td>\n",
       "      <td>0.996417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://news.livedoor.com/article/detail/609003...</td>\n",
       "      <td>sports-watch</td>\n",
       "      <td>sports-watch</td>\n",
       "      <td>0.999916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://news.livedoor.com/article/detail/668482...</td>\n",
       "      <td>peachy</td>\n",
       "      <td>peachy</td>\n",
       "      <td>0.999915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://news.livedoor.com/article/detail/470367...</td>\n",
       "      <td>sports-watch</td>\n",
       "      <td>sports-watch</td>\n",
       "      <td>0.999511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://news.livedoor.com/article/detail/496061...</td>\n",
       "      <td>sports-watch</td>\n",
       "      <td>sports-watch</td>\n",
       "      <td>0.995080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://news.livedoor.com/article/detail/565495...</td>\n",
       "      <td>sports-watch</td>\n",
       "      <td>sports-watch</td>\n",
       "      <td>0.995340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://news.livedoor.com/article/detail/690798...</td>\n",
       "      <td>peachy</td>\n",
       "      <td>peachy</td>\n",
       "      <td>0.999802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://news.livedoor.com/article/detail/555669...</td>\n",
       "      <td>sports-watch</td>\n",
       "      <td>sports-watch</td>\n",
       "      <td>0.999216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            contents        labels  \\\n",
       "0  http://news.livedoor.com/article/detail/474526...  sports-watch   \n",
       "1  http://news.livedoor.com/article/detail/591090...        peachy   \n",
       "2  http://news.livedoor.com/article/detail/491990...  sports-watch   \n",
       "3  http://news.livedoor.com/article/detail/609003...  sports-watch   \n",
       "4  http://news.livedoor.com/article/detail/668482...        peachy   \n",
       "5  http://news.livedoor.com/article/detail/470367...  sports-watch   \n",
       "6  http://news.livedoor.com/article/detail/496061...  sports-watch   \n",
       "7  http://news.livedoor.com/article/detail/565495...  sports-watch   \n",
       "8  http://news.livedoor.com/article/detail/690798...        peachy   \n",
       "9  http://news.livedoor.com/article/detail/555669...  sports-watch   \n",
       "\n",
       "        predict  confidence  \n",
       "0  sports-watch    0.988596  \n",
       "1        peachy    0.999831  \n",
       "2  sports-watch    0.996417  \n",
       "3  sports-watch    0.999916  \n",
       "4        peachy    0.999915  \n",
       "5  sports-watch    0.999511  \n",
       "6  sports-watch    0.995080  \n",
       "7  sports-watch    0.995340  \n",
       "8        peachy    0.999802  \n",
       "9  sports-watch    0.999216  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:\n",
      "peachy\n",
      "predict:\n",
      "peachy\n",
      "\n",
      "contents:\n",
      "http://news.livedoor.com/article/detail/5910900/2011-10-04T14:37:00+0900【終了しました】ムリをせず幸せになりたいオトナ女子へ。カウンセラー・五百田達成著『心のゆるめかた』を5名様にプレゼント「毎日充実しているけど、このままでいいか考えてしまう時がある」「特に理由もないのに、将来が漠然と不安」など、なんとなくモヤモヤするときはありませんか?　そのままズルズルと落ち込んでしまい、ブルーな気分から抜けだせないという経験は誰しも持っているのではないでしょうか。その原因のひとつとして、恋愛や仕事を成功させたいあまり、「頑張りすぎて空回っている」状態が挙げられます。焦りやプレッシャー、不安などから解放されて、もっと楽に毎日を過ごしたいですよね。そんなオトナ女子の心のお疲れモードを解消したいときにお勧めしたいのが、メディアでも話題の”オトナ女子カウンセラー”五百田達成が著した『心のゆるめかた』です。この本では、モヤモヤする気持ちは心が凝り固まっているからだとし、凝った心をほぐすためのヒントが紹介されています。すべての女子の胸の奥には、「幸せになる本能」が眠っています。・「理屈」よりも「気持ち」や「直感」を大事にしてる？・将来のことを考えすぎてない？・焦る気持ちでいっぱいいっぱいになってない？・ちゃんと自分を褒めてあげてる？「我慢してないのに愛される。頑張ってないのに夢が叶う」この本を読めば、そんな魔法のようなことが起きるのだそうです。そして、ゆるんだ心の明るいオーラは、人を惹きつける力があるのだとか。実際に読んだ方からは、・ずっと好きだった人からデートに誘われました（20代・営業職）・『だいじょうぶ』と言ってもらえて涙がこぼれました（40代・教師）・震災以降のモヤモヤがすーっと晴れました（30代・SE）などの嬉しい声が寄せられています。今回Peachyでは、この『心のゆるめかた』を5名様にプレゼントいたします。この機会に心の凝りをほぐして、頑張らずに幸せになれる心のゆるめ方をレッスンしてみてはいかがでしょうか。五百田達成著『心のゆるめかた』を5名様にプレゼント【賞品・応募数】五百田達成著『心のゆるめかた』（中経出版） / 5名様【プレゼント応募期間】2011年10月4日(火)〜2011年10月17日(月)【応募方法】下記『応募する』ボタンの応募フォームから、必要事項をご記入の上、ご応募ください。【当選発表】・当選については、厳正なる抽選の上、決定させていただきます。・当選は発送をもって発表とかえさせていただきます。・なお、賞品の発送は10月下旬頃を予定しております。・都合により、賞品の発送が多少遅れる場合もございます。あらかじめご了承ください。※本プレゼントは、終了しました。※■関連情報『心のゆるめかた』（中経出版）詳細はこちらから五百田達成オフィシャルブログ\n"
     ]
    }
   ],
   "source": [
    "print(\"labels:\")\n",
    "print(df.iloc[1][\"labels\"])\n",
    "print(\"predict:\")\n",
    "print(df.iloc[1][\"predict\"])\n",
    "print()\n",
    "print(\"contents:\")\n",
    "print(df.iloc[1][\"contents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
